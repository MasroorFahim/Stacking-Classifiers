# Stacking-Classifiers
So here, this is a simple demonstration of how a stacking classifiers work on a dataset.
Here in my experiment , first of all, the name of the dataset is Drebin Dataset and with this dataset initially it had like 213 features and 15037 rows with a combination of malware and benign which is denoted by 1 and 0 respectively in thje class column.
The procedure of my experiment is as follows:

1. Dataset Collection
2. Dataset Analysis and fixing
3. Dataset VarianceThreshold Detection
4. Pearson Correlation
5. Plotting box plot
6. Dataset Standardization (fit and transform)
7. Principal Component Analysis
8. Variance Inflation Factor
9. Feature Selection (Mutual Information)
10. For imbalance dataset using SMOTETomek with randomsampler
11. StratifiedShuffleSplit
12. Individual Evaluation of ML algorithms.
13. Stacking it up for further evaluation.

  The brief discussion of this will be:

1. So here, we first collect the dataset and verify if it sets up nicely with respect to the experiment.
   
2. Now comes the dataset analysis part. Here first of all the dataset is checked if theres any missing values in there. And if there is any missing valuem, it has to be fixed to analysis the dataset well.

   
3. Now the third part which is The VarianceThreshold is a feature selection method in machine learning, typically used for removing low-variance features from a dataset. Features with low variance generally contain little information, and removing them can be beneficial, especially in cases where there is redundancy or noise in the data.

Here is the full definition of VarianceThreshold:

VarianceThreshold in scikit-learn:
In scikit-learn, VarianceThreshold is a feature selection method provided in the feature_selection module. It operates on numerical features and removes those with variance below a certain threshold.

Key Components:
Variance:

Variance is a measure of the spread or dispersion of a set of values. In the context of feature selection, it refers to the amount of variability or change in the values of a feature across the samples in the dataset.
Threshold:

VarianceThreshold takes a threshold parameter, and it removes features with variance below this threshold. Features with variance less than the specified threshold are considered low-variance and are removed.

4. the fourth part is 
Pearson correlation, often referred to as Pearson's correlation coefficient or simply Pearson's r, is a statistical measure that quantifies the strength and direction of a linear relationship between two continuous variables. It is widely used in statistics to assess how well the relationship between two variables can be described by a straight line.

Here,
r=1 indicates a perfect positive linear relationship.
r=âˆ’1 indicates a perfect negative linear relationship.
r=0 indicates no linear relationship.

In our dataset, the columns or features which are 75 % correlated are removed by this special formulae.

5. After that , some plottings were conducted between the different features in order to know the insights.

6. After that , the dataset is standardized.
   
By
scaler = StandardScaler()
scaler.fit(X)
standardized_data = scaler.transform(X)

The mechanism of this is to make the scale of each feature to have a mean of 0 and a standard deviation of 1. Other transformations or scalers in scikit-learn can be used depending on the specific needs of your data.

7. The 7th step is Principal Component Analysis

Principal Component Analysis (PCA) is a dimensionality reduction technique widely used in machine learning and statistics. Its primary goal is to transform a high-dimensional dataset into a lower-dimensional representation while retaining as much of the original variance as possible. PCA achieves this by identifying the principal components, which are linear combinations of the original features, and ordering them by the amount of variance they capture. Generally, Each principal component represents a direction in feature space. The first principal component points in the direction of the maximum variance, the second principal component points in the direction of the second maximum variance, and so forth.PCA is often used for data exploration, noise reduction, and visualization. It can be applied to various domains, including image processing, genetics, and finance.

Key Concepts of PCA:

Variance Maximization:

PCA seeks to find the linear combinations (principal components) of the original features that maximize the variance in the data. The first principal component captures the most variance, the second principal component (orthogonal to the first) captures the second most variance, and so on.
Orthogonality:

Principal components are orthogonal to each other, meaning they are uncorrelated. This ensures that each component contributes uniquely to the overall variance.
Eigenvalue Decomposition:

PCA involves the eigenvalue decomposition of the covariance matrix of the original features. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance captured by each component.
Dimensionality Reduction:

By selecting a subset of the principal components that capture a significant amount of variance, one can create a lower-dimensional representation of the data. This reduction in dimensionality can lead to simplified models, faster training times, and improved interpretability.
Principal Component Scores:

The transformed data, called the principal component scores, is obtained by projecting the original data onto the subspace spanned by the selected principal components.

8. After PCA, we did The Variance Inflation Factor (VIF) which is a statistical measure used to assess the severity of multicollinearity in a regression analysis. Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, making it challenging to isolate the individual effect of each variable on the dependent variable.

In our experiment, we took every features which had a vif factor of less than 7.

9. The later step is Mutual Information feature selection method.
    
Mutual Information (MI) is a statistical metric used in feature selection to quantify the relationship between two variables by measuring the amount of information obtained about one variable through the observation of the other. In the context of feature selection, mutual information is often employed to evaluate the relevance of individual features with respect to the target variable.

Generally, Feature Selection Using Mutual Information:
In the context of machine learning, mutual information can be employed for feature selection by ranking features based on their individual mutual information scores with the target variable. Features with higher scores are considered more informative and are selected for inclusion in the model.

In our experiment, we took the best 70 features using mutual informationn and with the help of SelectKBest, which is a feature selection technique in scikit-learn that is used to select the top k features based on a specified scoring function. This method is part of the feature selection module (sklearn.feature_selection) in scikit-learn and is commonly employed to improve the performance of machine learning models by focusing on the most relevant features.

10. Using SMOTETOMEK and randomsampler to address the dataset's imbalance is the tenth step.
    
Dealing with imbalanced datasets is a common challenge in machine learning. Two popular techniques for handling imbalanced datasets are using random sampling (RandomSampler) and SMOTE (Synthetic Minority Over-sampling Technique) combined with Tomek links (SMOTETomek).

1. RandomSampler:
The RandomSampler is a simple approach to balance a dataset by randomly under-sampling the majority class. This technique involves removing some instances from the majority class to make the class distribution more balanced.

2. SMOTETomek:
SMOTETomek is a combination of over-sampling the minority class using SMOTE and under-sampling the majority class using Tomek links. The goal is to generate synthetic samples for the minority class and remove potentially noisy instances from both classes.

Both RandomSampler and SMOTETomek are available in the imbalanced-learn library (imblearn). Before using these techniques, it's important to assess the specific characteristics of your dataset and choose the method that suits your problem. Keep in mind that no single technique is universally best for all imbalanced datasets, and experimentation is often necessary. Additionally, the performance of these methods may depend on the algorithm used for classification.

11. The dataset is then again StratifiedShuffle and spliited in to the test size of 20 percent and train size of 80 percent. And we used this expression, sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

12. Then the individual evaluation of ml algorithm starts. Here, we evaluated the algorithms are accuracy, precision, specificity ,f1 score, recall and plotted confusion matrix accordingly to access every matrix for this dataset.

In here, 13 different classifiers were taken and evaluated accordingly and they are hypertuned first to get an accurate view.

1. Decision Tree classifier
2. KNN classifier
3. Logistic regression classifier
4. Svm classifier
5. Naive-Bayes Classifier
6. RandomForestClassifier
7. SGDClassifier
8. XGBoostClassifier
9. PassiveAggressiveClassifier
10. ExtraTreesClassifier
11. Perceptron Classifier
12. LGBMClassifier
13. RidgeClassifier

A short Description of each of them are given below:

1. Decision Tree Classifier:
A Decision Tree is a tree-shaped model of decisions where each node represents a decision based on a particular feature. It recursively splits the dataset into subsets based on the most significant feature, creating a tree-like structure to make predictions.

2. K-Nearest Neighbors (KNN) Classifier:
KNN is a non-parametric and instance-based algorithm for classification. It classifies new instances based on the majority class of their k nearest neighbors in the feature space. The choice of 'k' determines the number of neighbors to consider.

3. Logistic Regression Classifier:
Despite its name, logistic regression is a linear model for binary classification. It uses the logistic function to model the probability that a given instance belongs to a particular class. It's widely used and interpretable.

4. Support Vector Machine (SVM) Classifier:
SVM is a supervised machine learning algorithm used for classification and regression tasks. It finds the optimal hyperplane that separates classes in a high-dimensional space, maximizing the margin between classes.

5. Naive-Bayes Classifier:
Naive-Bayes is a probabilistic classification algorithm based on Bayes' theorem. It assumes that features are conditionally independent, given the class label. It's particularly efficient for text classification and simple problems.

6. RandomForestClassifier:
RandomForest is an ensemble learning method that constructs a multitude of decision trees during training. It combines their predictions to improve accuracy and reduce overfitting.

7. SGDClassifier (Stochastic Gradient Descent Classifier):
SGDClassifier is a linear classifier that uses stochastic gradient descent as an optimization algorithm. It's particularly useful for large datasets and online learning scenarios.

8. XGBoostClassifier:
XGBoost is an efficient and scalable implementation of gradient boosting. It is an ensemble learning algorithm that builds a series of weak learners (usually decision trees) and combines them to create a strong learner.

9. PassiveAggressiveClassifier:
The Passive-Aggressive algorithm is an online learning algorithm for classification. It is suitable for situations where the data is not static, and the model needs to adapt to changes over time.

10. ExtraTreesClassifier:
ExtraTrees (Extremely Randomized Trees) is an ensemble learning method that builds multiple decision trees and selects the splits for nodes randomly. This randomness can often lead to improved performance.

11. Perceptron Classifier:
A perceptron is a simple neural network model that learns a binary linear classifier. It's a single-layer neural network with a threshold activation function.

12. LGBMClassifier (LightGBM Classifier):
LightGBM is a gradient boosting framework that uses tree-based learning algorithms. It's designed for distributed and efficient training with a focus on handling large datasets.

13. RidgeClassifier:
RidgeClassifier is a linear classifier that uses Ridge Regression, a regularized linear regression model. It adds a penalty term to the least squares objective, promoting models with lower complexity.
Each of these classifiers has its strengths and weaknesses, and the choice depends on the characteristics of the data and the specific requirements of the problem at hand.


13. The last step is to use stacking classifier by taking stacks of this previous used classifiers and combining them to see the results together how they work.

A general overview of stacking classifier will be :

Stacking, short for stacked generalization, is an ensemble learning technique that involves combining multiple base classifiers to create a more robust and potentially higher-performing model. Stacking goes beyond simple ensembling methods like bagging and boosting by introducing a meta-learner that learns to combine the predictions of the base classifiers. The idea is to leverage the diverse strengths of different models to improve overall predictive performance.

Key Components of Stacking:

Base Classifiers:

These are the individual classifiers or models that form the base layer of the stacking ensemble. They can be diverse in terms of algorithms, hyperparameters, or training data. Common choices include decision trees, support vector machines, neural networks, etc.
Meta-Learner (or Blender):

The meta-learner is a higher-level model that takes the predictions from the base classifiers as input features and learns to make the final prediction. It can be a simple model like logistic regression or a more complex model like another decision tree or an ensemble method.

Training Process:

The training process in stacking typically involves the following steps:
The base classifiers are trained on the original dataset.
The predictions of the base classifiers are used as features to train the meta-learner.
The meta-learner is trained to combine the base classifiers' predictions effectively.
Prediction Process:

For making predictions on new, unseen data, the base classifiers generate predictions, and these predictions are then fed into the trained meta-learner to produce the final ensemble prediction.

So, this is a fun project and i jhope al ot of people will get benefit from this .

THANK YOU.

